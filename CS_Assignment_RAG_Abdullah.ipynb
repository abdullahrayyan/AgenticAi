{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Project Name: Buffett-Wisdom RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Student Name: Abdullah Rayyan\n",
        "\n",
        "System ID: 2023001064\n",
        "\n",
        "Course: B.Tech CSE (3rd Year), Sharda University\n",
        "\n",
        "Subject: AI / Machine Learning Project\n"
      ],
      "metadata": {
        "id": "aku-WFME90vJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EisOG5m1lziH"
      },
      "outputs": [],
      "source": [
        "# Install LangChain, OpenAI (for LLM), and ChromaDB (Vector Store)\n",
        "!pip install -q langchain langchain-community langchain-openai chromadb pypdf tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Load the PDF (Make sure the filename matches what you uploaded)\n",
        "loader = PyPDFLoader(\"2023ltr.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# 2. Split the text into chunks\n",
        "# Assignment Requirement: \"Reason for chosen strategy\"\n",
        "# We use RecursiveCharacterTextSplitter because it tries to keep paragraphs\n",
        "# and sentences together, preserving the context better than a simple split.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(data)\n",
        "\n",
        "print(f\"Total chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "ihm6YzFX1DtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-text-splitters"
      ],
      "metadata": {
        "id": "yJa2087S1eEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective: To develop a robust RAG system capable of extracting and synthesizing financial insights from dense corporate documents. Specifically, this application parses Warren Buffettâ€™s annual shareholder letters to provide accurate, context-aware answers to complex investment queries.\n",
        "\n",
        "The Challenge: Traditional LLMs often suffer from \"hallucinations\" or lack up-to-date specific data. By implementing RAG, we ground the model's responses in a specific knowledge base (PDF), ensuring factual accuracy and reducing computational costs."
      ],
      "metadata": {
        "id": "x-BYzlIa_DLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "# This is the updated import path\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Load the PDF\n",
        "# IMPORTANT: Ensure '2023ltr.pdf' is actually uploaded in the folder icon on the left!\n",
        "loader = PyPDFLoader(\"2023ltr.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# 2. Split the text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(data)\n",
        "\n",
        "print(f\"Total chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "CCkBd8cW1jtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# 1. Initialize the Embedding Model\n",
        "# Requirement: \"Embedding model used\" -> OpenAI text-embedding-3-small\n",
        "# Reason: It is highly cost-effective and creates dense, accurate vectors.\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 2. Create the Vector Database\n",
        "# This takes your chunks, turns them into numbers, and saves them.\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(\"Vector Database 'ChromaDB' is ready and persisted!\")"
      ],
      "metadata": {
        "id": "Cdff7K4-10bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "FG6G-r6w2WHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The application follows a standard modular RAG pipeline:\n",
        "\n",
        "Data Ingestion: Loading PDF documents using PyPDFLoader.\n",
        "\n",
        "Preprocessing: Fragmenting text using RecursiveCharacterTextSplitter to maintain semantic flow (Chunk size: 1000, Overlap: 100).\n",
        "\n",
        "Vectorization: Generating high-dimensional embeddings using the HuggingFace all-MiniLM-L6-v2 model.\n",
        "\n",
        "Storage: Utilizing ChromaDB as the vector store for efficient similarity searching.\n",
        "\n",
        "Retrieval: Fetching relevant context via the LangChain VectorStoreRetriever.\n",
        "\n",
        "Generation: Using Mistral-7B-Instruct-v0.2 (local pipeline) to generate the final response based on the retrieved context."
      ],
      "metadata": {
        "id": "e5cUSJXO_OmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Use a free, high-quality open-source model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create the Vector Database exactly as before\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(\"Vector Database created for FREE using HuggingFace!\")"
      ],
      "metadata": {
        "id": "xfCvDbQm2bAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-huggingface huggingface_hub"
      ],
      "metadata": {
        "id": "2MBedEzM4LB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Set your Hugging Face Token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# 2. Initialize the Free LLM (Mistral-7B is excellent for RAG)\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1, # Low temperature for factual answers\n",
        "    huggingfacehub_api_token=os.environ[\"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\"]\n",
        ")\n",
        "\n",
        "# 3. Create the RAG Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        ")\n",
        "\n",
        "# 4. Assignment Requirement: 3 Test Queries\n",
        "queries = [\n",
        "    \"What is Warren Buffett's advice on investing for the long term?\",\n",
        "    \"What does the document say about Berkshire's insurance business?\",\n",
        "    \"How does Buffett define a 'wonderful' company?\"\n",
        "]\n",
        "\n",
        "for i, q in enumerate(queries, 1):\n",
        "    print(f\"\\n--- Query {i} ---\")\n",
        "    print(f\"Question: {q}\")\n",
        "    response = qa_chain.invoke(q)\n",
        "    print(f\"Answer: {response['result']}\")"
      ],
      "metadata": {
        "id": "eHwcIen84Rbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-classic\n"
      ],
      "metadata": {
        "id": "ehdLhuTs4uCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "# Change from langchain.chains to langchain_classic.chains\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "\n",
        "# 1. Set your Hugging Face Token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# 2. Initialize the Free LLM (Mistral-7B is excellent for RAG)\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1, # Low temperature for factual answers\n",
        "    huggingfacehub_api_token=os.environ[\"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\"]\n",
        ")\n",
        "\n",
        "# 3. Create the RAG Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        ")\n",
        "\n",
        "# 4. Assignment Requirement: 3 Test Queries\n",
        "queries = [\n",
        "    \"What is Warren Buffett's advice on investing for the long term?\",\n",
        "    \"What does the document say about Berkshire's insurance business?\",\n",
        "    \"How does Buffett define a 'wonderful' company?\"\n",
        "]\n",
        "\n",
        "for i, q in enumerate(queries, 1):\n",
        "    print(f\"\\n--- Query {i} ---\")\n",
        "    print(f\"Question: {q}\")\n",
        "    response = qa_chain.invoke(q)\n",
        "    print(f\"Answer: {response['result']}\")"
      ],
      "metadata": {
        "id": "BfdINHNi4-wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Set your token correctly\n",
        "# Replace 'your_token_here' with: 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\n",
        "my_hf_token = \"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\"\n",
        "\n",
        "# 2. Initialize the Free LLM\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=my_hf_token  # Pass the variable directly here\n",
        ")\n",
        "\n",
        "# 3. Create the RAG Chain (Using the updated create_retrieval_chain method)\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define a simple prompt for the AI\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based only on the provided context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "# Combine everything\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), document_chain)\n",
        "\n",
        "# 4. Run your test query\n",
        "response = rag_chain.invoke({\"input\": \"What is Warren Buffett's advice on investing for the long term?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "id": "HF_LoCKI5cY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-classic"
      ],
      "metadata": {
        "id": "JOcWYfTZ6Lms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. FIXED IMPORTS FOR 2026\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 2. DEFINE THE SYSTEM PROMPT\n",
        "system_prompt = (\n",
        "    \"You are a professional assistant. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"\\n\\n\"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. BUILD THE RAG CHAIN\n",
        "# This uses the 'llm' and 'vector_db' you created in previous steps\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "# 4. RUN YOUR TEST QUERIES\n",
        "test_queries = [\n",
        "    \"What is Warren Buffett's advice on investing for the long term?\",\n",
        "    \"What does the document say about Berkshire's insurance business?\",\n",
        "    \"How does Buffett define a 'wonderful' company?\"\n",
        "]\n",
        "\n",
        "print(\"--- GENERATING ANSWERS ---\")\n",
        "for q in test_queries:\n",
        "    response = rag_chain.invoke({\"input\": q})\n",
        "    print(f\"\\nQuestion: {q}\")\n",
        "    print(f\"Answer: {response['answer']}\")"
      ],
      "metadata": {
        "id": "qVQ23onj6cQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP LLM & EMBEDDINGS (This fixes the NameError)\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# AUTHENTICATION\n",
        "my_hf_token = \"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\"\n",
        "\n",
        "# 1. UPDATED LLM DEFINITION (Fixes the task mismatch error)\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    task=\"text-generation\",  # This is the key fix\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=my_hf_token\n",
        ")\n",
        "\n",
        "# 2. RUN YOUR TEST AGAIN\n",
        "# (Ensure your vector_db and prompt are already defined)\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "response = rag_chain.invoke({\"input\": \"What is the main advice in this document?\"})\n",
        "print(f\"Answer: {response['answer']}\")\n",
        "\n",
        "# DEFINE EMBEDDINGS (Matches your previous step)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# RE-LOAD VECTOR DB (Points to the directory you created earlier)\n",
        "vector_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "\n",
        "# 2. BUILD THE RAG CHAIN\n",
        "system_prompt = (\n",
        "    \"You are a professional assistant. Use the following context to answer. \"\n",
        "    \"If you don't know, say you don't know.\\n\\nContext: {context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "# 3. RUN TEST\n",
        "response = rag_chain.invoke({\"input\": \"What is the main advice in this document?\"})\n",
        "print(f\"Answer: {response['answer']}\")"
      ],
      "metadata": {
        "id": "-_7UcZ7b6xNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. INSTALL UPDATED INTEGRATIONS\n",
        "!pip install -q -U langchain-huggingface langchain-chroma langchain-classic\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 2. SETUP (Replace with your token)\n",
        "my_hf_token = \"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\"\n",
        "\n",
        "# Use the ChatHuggingFace wrapper to fix the 'conversational' task error\n",
        "base_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=my_hf_token\n",
        ")\n",
        "llm = ChatHuggingFace(llm=base_llm)\n",
        "\n",
        "# Load Embeddings & Vector DB\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vector_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "\n",
        "# 3. DEFINE RAG LOGIC\n",
        "system_prompt = (\n",
        "    \"You are a helpful assistant. Use the following context to answer the question. \"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Create the Modern Chain\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "# 4. EXECUTE & TEST\n",
        "print(\"--- RUNNING ASSIGNMENT QUERIES ---\")\n",
        "query = \"What is the main investing advice in this document?\"\n",
        "response = rag_chain.invoke({\"input\": query})\n",
        "print(f\"Answer: {response['answer']}\")"
      ],
      "metadata": {
        "id": "tdf43J5Z7XWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FINAL WORKING RAG CHAIN (The Fix)\n",
        "# ==========================================\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Fix for the 'stream' argument error\n",
        "base_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=my_hf_token, # Uses the token from your previous cell\n",
        ")\n",
        "\n",
        "# Explicitly disabling streaming solves the Post() keyword error\n",
        "llm = ChatHuggingFace(llm=base_llm, disable_streaming=True)\n",
        "\n",
        "# Re-link the components\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "# Run the final assignment tests\n",
        "print(\"--- RUNNING ASSIGNMENT QUERIES ---\")\n",
        "query = \"What is the main investing advice in this document?\"\n",
        "response = rag_chain.invoke({\"input\": query})\n",
        "print(f\"Answer: {response['answer']}\")"
      ],
      "metadata": {
        "id": "p5i6r62v7sZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FINAL WORKING RAG CHAIN (Fix for 'stream' error)\n",
        "# ==========================================\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# 1. Base LLM Setup\n",
        "base_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=my_hf_token, # Uses the token you defined earlier\n",
        ")\n",
        "\n",
        "# 2. Chat Wrapper with STREAMING DISABLED\n",
        "# This 'disable_streaming=True' is the key fix for the TypeError you received.\n",
        "llm = ChatHuggingFace(llm=base_llm, disable_streaming=True)\n",
        "\n",
        "# 3. RE-BUILD THE CHAIN\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "# 4. RUN FINAL TEST\n",
        "print(\"--- RUNNING ASSIGNMENT QUERIES ---\")\n",
        "query = \"What is the main investing advice in this document?\"\n",
        "response = rag_chain.invoke({\"input\": query})\n",
        "print(f\"Answer: {response['answer']}\")"
      ],
      "metadata": {
        "id": "x6p2D1af8AY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate"
      ],
      "metadata": {
        "id": "y3FAw3aO8OA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# 1. SETUP LOCAL LLM PIPELINE\n",
        "# This avoids the 'Client.post' API error entirely\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Initialize the transformers pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "# Wrap it for LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# 2. RE-BUILD THE RAG CHAIN\n",
        "# (Using your existing 'prompt' and 'vector_db' from previous cells)\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n",
        "\n",
        "# 3. RUN THE TEST\n",
        "print(\"--- RUNNING ASSIGNMENT QUERIES ---\")\n",
        "query = \"What is the main investing advice in this document?\"\n",
        "response = rag_chain.invoke({\"input\": query})\n",
        "print(f\"Answer: {response['answer']}\")"
      ],
      "metadata": {
        "id": "-daMV6Ib8VAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Query Retrieval: Implementing query expansion to improve the diversity of retrieved documents.\n",
        "\n",
        "Reranking: Adding a Cross-Encoder reranker to ensure the most relevant chunks are prioritized for the LLM.\n",
        "\n",
        "Integration: Deploying the model via a Streamlit or Gradio web interface for user interaction."
      ],
      "metadata": {
        "id": "-2x70Q8E_YUo"
      }
    }
  ]
}